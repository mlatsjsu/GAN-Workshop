{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114d69b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 42\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"data/celeba\"# CHANGE THIS TO MAKE IT ACCURATE\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 16\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/celeba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4e0313a9f192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                            ]))\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     91\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m     92\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Faster and available in Python 3.5 and above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/celeba'"
     ]
    }
   ],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3f1eb13a8ba4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnetG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Handle multi-gpu if desired\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mngpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the generator\n",
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7bb027a5120e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the Discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnetD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Handle multi-gpu if desired\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mngpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the weights_init function to randomly initialize all weights\n",
    "#  to mean=0, stdev=0.2.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e64f5e70c9fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create batch of latent vectors that we will use to visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#  the progression of the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfixed_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Establish convention for real and fake labels during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e2486cac69e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# For each batch in the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\"\n",
       "href=\"https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/\n",
       "css/font-awesome.min.css\">\n",
       "<script language=\"javascript\">\n",
       "  function isInternetExplorer() {\n",
       "    ua = navigator.userAgent;\n",
       "    /* MSIE used to detect old browsers and Trident used to newer ones*/\n",
       "    return ua.indexOf(\"MSIE \") > -1 || ua.indexOf(\"Trident/\") > -1;\n",
       "  }\n",
       "\n",
       "  /* Define the Animation class */\n",
       "  function Animation(frames, img_id, slider_id, interval, loop_select_id){\n",
       "    this.img_id = img_id;\n",
       "    this.slider_id = slider_id;\n",
       "    this.loop_select_id = loop_select_id;\n",
       "    this.interval = interval;\n",
       "    this.current_frame = 0;\n",
       "    this.direction = 0;\n",
       "    this.timer = null;\n",
       "    this.frames = new Array(frames.length);\n",
       "\n",
       "    for (var i=0; i<frames.length; i++)\n",
       "    {\n",
       "     this.frames[i] = new Image();\n",
       "     this.frames[i].src = frames[i];\n",
       "    }\n",
       "    var slider = document.getElementById(this.slider_id);\n",
       "    slider.max = this.frames.length - 1;\n",
       "    if (isInternetExplorer()) {\n",
       "        // switch from oninput to onchange because IE <= 11 does not conform\n",
       "        // with W3C specification. It ignores oninput and onchange behaves\n",
       "        // like oninput. In contrast, Mircosoft Edge behaves correctly.\n",
       "        slider.setAttribute('onchange', slider.getAttribute('oninput'));\n",
       "        slider.setAttribute('oninput', null);\n",
       "    }\n",
       "    this.set_frame(this.current_frame);\n",
       "  }\n",
       "\n",
       "  Animation.prototype.get_loop_state = function(){\n",
       "    var button_group = document[this.loop_select_id].state;\n",
       "    for (var i = 0; i < button_group.length; i++) {\n",
       "        var button = button_group[i];\n",
       "        if (button.checked) {\n",
       "            return button.value;\n",
       "        }\n",
       "    }\n",
       "    return undefined;\n",
       "  }\n",
       "\n",
       "  Animation.prototype.set_frame = function(frame){\n",
       "    this.current_frame = frame;\n",
       "    document.getElementById(this.img_id).src =\n",
       "            this.frames[this.current_frame].src;\n",
       "    document.getElementById(this.slider_id).value = this.current_frame;\n",
       "  }\n",
       "\n",
       "  Animation.prototype.next_frame = function()\n",
       "  {\n",
       "    this.set_frame(Math.min(this.frames.length - 1, this.current_frame + 1));\n",
       "  }\n",
       "\n",
       "  Animation.prototype.previous_frame = function()\n",
       "  {\n",
       "    this.set_frame(Math.max(0, this.current_frame - 1));\n",
       "  }\n",
       "\n",
       "  Animation.prototype.first_frame = function()\n",
       "  {\n",
       "    this.set_frame(0);\n",
       "  }\n",
       "\n",
       "  Animation.prototype.last_frame = function()\n",
       "  {\n",
       "    this.set_frame(this.frames.length - 1);\n",
       "  }\n",
       "\n",
       "  Animation.prototype.slower = function()\n",
       "  {\n",
       "    this.interval /= 0.7;\n",
       "    if(this.direction > 0){this.play_animation();}\n",
       "    else if(this.direction < 0){this.reverse_animation();}\n",
       "  }\n",
       "\n",
       "  Animation.prototype.faster = function()\n",
       "  {\n",
       "    this.interval *= 0.7;\n",
       "    if(this.direction > 0){this.play_animation();}\n",
       "    else if(this.direction < 0){this.reverse_animation();}\n",
       "  }\n",
       "\n",
       "  Animation.prototype.anim_step_forward = function()\n",
       "  {\n",
       "    this.current_frame += 1;\n",
       "    if(this.current_frame < this.frames.length){\n",
       "      this.set_frame(this.current_frame);\n",
       "    }else{\n",
       "      var loop_state = this.get_loop_state();\n",
       "      if(loop_state == \"loop\"){\n",
       "        this.first_frame();\n",
       "      }else if(loop_state == \"reflect\"){\n",
       "        this.last_frame();\n",
       "        this.reverse_animation();\n",
       "      }else{\n",
       "        this.pause_animation();\n",
       "        this.last_frame();\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  Animation.prototype.anim_step_reverse = function()\n",
       "  {\n",
       "    this.current_frame -= 1;\n",
       "    if(this.current_frame >= 0){\n",
       "      this.set_frame(this.current_frame);\n",
       "    }else{\n",
       "      var loop_state = this.get_loop_state();\n",
       "      if(loop_state == \"loop\"){\n",
       "        this.last_frame();\n",
       "      }else if(loop_state == \"reflect\"){\n",
       "        this.first_frame();\n",
       "        this.play_animation();\n",
       "      }else{\n",
       "        this.pause_animation();\n",
       "        this.first_frame();\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  Animation.prototype.pause_animation = function()\n",
       "  {\n",
       "    this.direction = 0;\n",
       "    if (this.timer){\n",
       "      clearInterval(this.timer);\n",
       "      this.timer = null;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  Animation.prototype.play_animation = function()\n",
       "  {\n",
       "    this.pause_animation();\n",
       "    this.direction = 1;\n",
       "    var t = this;\n",
       "    if (!this.timer) this.timer = setInterval(function() {\n",
       "        t.anim_step_forward();\n",
       "    }, this.interval);\n",
       "  }\n",
       "\n",
       "  Animation.prototype.reverse_animation = function()\n",
       "  {\n",
       "    this.pause_animation();\n",
       "    this.direction = -1;\n",
       "    var t = this;\n",
       "    if (!this.timer) this.timer = setInterval(function() {\n",
       "        t.anim_step_reverse();\n",
       "    }, this.interval);\n",
       "  }\n",
       "</script>\n",
       "\n",
       "<style>\n",
       ".animation {\n",
       "    display: inline-block;\n",
       "    text-align: center;\n",
       "}\n",
       "input[type=range].anim-slider {\n",
       "    width: 374px;\n",
       "    margin-left: auto;\n",
       "    margin-right: auto;\n",
       "}\n",
       ".anim-buttons {\n",
       "    margin: 8px 0px;\n",
       "}\n",
       ".anim-buttons button {\n",
       "    padding: 0;\n",
       "    width: 36px;\n",
       "}\n",
       ".anim-state label {\n",
       "    margin-right: 8px;\n",
       "}\n",
       ".anim-state input {\n",
       "    margin: 0;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<div class=\"animation\">\n",
       "  <img id=\"_anim_imgc4b69a2bd4b64eb29cf86757b15dd6a6\">\n",
       "  <div class=\"anim-controls\">\n",
       "    <input id=\"_anim_sliderc4b69a2bd4b64eb29cf86757b15dd6a6\" type=\"range\" class=\"anim-slider\"\n",
       "           name=\"points\" min=\"0\" max=\"1\" step=\"1\" value=\"0\"\n",
       "           oninput=\"animc4b69a2bd4b64eb29cf86757b15dd6a6.set_frame(parseInt(this.value));\"></input>\n",
       "    <div class=\"anim-buttons\">\n",
       "      <button onclick=\"animc4b69a2bd4b64eb29cf86757b15dd6a6.slower()\"><i class=\"fa fa-minus\"></i></button>\n",
       "      <button onclick=\"animc4b69a2bd4b64eb29cf86757b15dd6a6.first_frame()\"><i class=\"fa fa-fast-backward\">\n",
       "          </i></button>\n",
       "      <button onclick=\"animc4b69a2bd4b64eb29cf86757b15dd6a6.previous_frame()\">\n",
       "          <i class=\"fa fa-step-backward\"></i></button>\n",
       "      <button onclick=\"animc4b69a2bd4b64eb29cf86757b15dd6a6.reverse_animation()\">\n",
       "          <i class=\"fa fa-play fa-flip-horizontal\"></i></button>\n",
       "      <button onclick=\"animc4b69a2bd4b64eb29cf86757b15dd6a6.pause_animation()\"><i class=\"fa fa-pause\">\n",
       "          </i></button>\n",
       "      <button onclick=\"animc4b69a2bd4b64eb29cf86757b15dd6a6.play_animation()\"><i class=\"fa fa-play\"></i>\n",
       "          </button>\n",
       "      <button onclick=\"animc4b69a2bd4b64eb29cf86757b15dd6a6.next_frame()\"><i class=\"fa fa-step-forward\">\n",
       "          </i></button>\n",
       "      <button onclick=\"animc4b69a2bd4b64eb29cf86757b15dd6a6.last_frame()\"><i class=\"fa fa-fast-forward\">\n",
       "          </i></button>\n",
       "      <button onclick=\"animc4b69a2bd4b64eb29cf86757b15dd6a6.faster()\"><i class=\"fa fa-plus\"></i></button>\n",
       "    </div>\n",
       "    <form action=\"#n\" name=\"_anim_loop_selectc4b69a2bd4b64eb29cf86757b15dd6a6\" class=\"anim-state\">\n",
       "      <input type=\"radio\" name=\"state\" value=\"once\" id=\"_anim_radio1_c4b69a2bd4b64eb29cf86757b15dd6a6\"\n",
       "             >\n",
       "      <label for=\"_anim_radio1_c4b69a2bd4b64eb29cf86757b15dd6a6\">Once</label>\n",
       "      <input type=\"radio\" name=\"state\" value=\"loop\" id=\"_anim_radio2_c4b69a2bd4b64eb29cf86757b15dd6a6\"\n",
       "             checked>\n",
       "      <label for=\"_anim_radio2_c4b69a2bd4b64eb29cf86757b15dd6a6\">Loop</label>\n",
       "      <input type=\"radio\" name=\"state\" value=\"reflect\" id=\"_anim_radio3_c4b69a2bd4b64eb29cf86757b15dd6a6\"\n",
       "             >\n",
       "      <label for=\"_anim_radio3_c4b69a2bd4b64eb29cf86757b15dd6a6\">Reflect</label>\n",
       "    </form>\n",
       "  </div>\n",
       "</div>\n",
       "\n",
       "\n",
       "<script language=\"javascript\">\n",
       "  /* Instantiate the Animation class. */\n",
       "  /* The IDs given should match those used in the template above. */\n",
       "  (function() {\n",
       "    var img_id = \"_anim_imgc4b69a2bd4b64eb29cf86757b15dd6a6\";\n",
       "    var slider_id = \"_anim_sliderc4b69a2bd4b64eb29cf86757b15dd6a6\";\n",
       "    var loop_select_id = \"_anim_loop_selectc4b69a2bd4b64eb29cf86757b15dd6a6\";\n",
       "    var frames = new Array(0);\n",
       "    \n",
       "\n",
       "\n",
       "    /* set a timeout to make sure all the above elements are created before\n",
       "       the object is initialized. */\n",
       "    setTimeout(function() {\n",
       "        animc4b69a2bd4b64eb29cf86757b15dd6a6 = new Animation(frames, img_id, slider_id, 1000.0,\n",
       "                                 loop_select_id);\n",
       "    }, 0);\n",
       "  })()\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%capture\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-90391afba428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Grab a batch of real images from the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreal_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot the real images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
